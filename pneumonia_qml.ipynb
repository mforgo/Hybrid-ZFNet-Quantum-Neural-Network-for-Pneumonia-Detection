{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3029006",
   "metadata": {
    "id": "e3029006"
   },
   "source": [
    "# Hybrid model for pneumonia detection\n",
    "## 1. Setup\n",
    "### 1.1. Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2364b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47491,
     "status": "ok",
     "timestamp": 1760344336272,
     "user": {
      "displayName": "Michal ForgÃ³",
      "userId": "03385761472197084991"
     },
     "user_tz": -120
    },
    "id": "b2364b28",
    "outputId": "6fd1cfb2-d376-4484-b1de-cb14b666b760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane\n",
      "  Downloading pennylane-0.43.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Collecting pennylane-qiskit\n",
      "  Downloading pennylane_qiskit-0.43.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.6.1)\n",
      "Collecting rustworkx>=0.14.0 (from pennylane)\n",
      "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
      "Collecting appdirs (from pennylane)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting autoray==0.8.0 (from pennylane)\n",
      "  Downloading autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (6.2.4)\n",
      "Collecting pennylane-lightning>=0.43 (from pennylane)\n",
      "  Downloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
      "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
      "Collecting diastatic-malt (from pennylane)\n",
      "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Collecting qiskit<2.2,>=2.0 (from pennylane-qiskit)\n",
      "  Downloading qiskit-2.1.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting qiskit-aer~=0.17.1 (from pennylane-qiskit)\n",
      "  Downloading qiskit_aer-0.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
      "Collecting qiskit-ibm-runtime~=0.41.1 (from pennylane-qiskit)\n",
      "  Downloading qiskit_ibm_runtime-0.41.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from pennylane-qiskit) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.12.12)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.43->pennylane)\n",
      "  Downloading scipy_openblas32-0.3.30.359.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.2,>=2.0->pennylane-qiskit) (0.3.8)\n",
      "Collecting stevedore>=3.0.0 (from qiskit<2.2,>=2.0->pennylane-qiskit)\n",
      "  Downloading stevedore-5.6.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer~=0.17.1->pennylane-qiskit) (5.9.5)\n",
      "Collecting requests-ntlm>=1.1.0 (from qiskit-ibm-runtime~=0.41.1->pennylane-qiskit)\n",
      "  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (2.5.0)\n",
      "Collecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime~=0.41.1->pennylane-qiskit)\n",
      "  Downloading ibm_platform_services-0.72.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (2.12.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->pennylane-qiskit) (1.3.0)\n",
      "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
      "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.7.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Collecting ibm_cloud_sdk_core<4.0.0,>=3.24.2 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit)\n",
      "  Downloading ibm_cloud_sdk_core-3.24.2-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.5.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.5.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.5.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.12/dist-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (43.0.3)\n",
      "Collecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit)\n",
      "  Downloading pyspnego-0.12.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (2.0.0)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from ibm_cloud_sdk_core<4.0.0,>=3.24.2->ibm-platform-services>=0.22.6->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (2.10.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime~=0.41.1->pennylane-qiskit) (2.23)\n",
      "Downloading pennylane-0.43.2-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading autoray-0.8.0-py3-none-any.whl (934 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pennylane_qiskit-0.43.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qiskit-2.1.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading qiskit_aer-0.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading qiskit_ibm_runtime-0.41.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ibm_platform_services-0.72.0-py3-none-any.whl (378 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\n",
      "Downloading scipy_openblas32-0.3.30.359.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading stevedore-5.6.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ibm_cloud_sdk_core-3.24.2-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyspnego-0.12.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: appdirs, stevedore, scipy-openblas32, rustworkx, autoray, qiskit, ibm_cloud_sdk_core, diastatic-malt, qiskit-aer, pyspnego, ibm-platform-services, requests-ntlm, qiskit-ibm-runtime, pennylane-lightning, pennylane, pennylane-qiskit\n",
      "Successfully installed appdirs-1.4.4 autoray-0.8.0 diastatic-malt-2.15.2 ibm-platform-services-0.72.0 ibm_cloud_sdk_core-3.24.2 pennylane-0.43.2 pennylane-lightning-0.43.0 pennylane-qiskit-0.43.0 pyspnego-0.12.0 qiskit-2.1.2 qiskit-aer-0.17.2 qiskit-ibm-runtime-0.41.1 requests-ntlm-1.3.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.359.2 stevedore-5.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane scikit-learn numpy scipy matplotlib pandas pennylane-qiskit kagglehub scikit-image seaborn pillow opencv-python torch torchvision pennylane-lightning-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b322fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”’ Global seed set to 6\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # Experiment Metadata\n",
    "    project_name: str = \"Hybrid_ResNet50_QNN_Pneumonia\"\n",
    "    seed: int = 6\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Data Paths\n",
    "    data_root: str = \"/home/mforgo/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray/\"\n",
    "    output_dir: str = \"./results\"\n",
    "    \n",
    "    # Classical Backbone\n",
    "    backbone_name: str = \"resnet50\"  # Fixed naming consistencyd\n",
    "    feature_dim: int = 2048          # 2048 for ResNet50\n",
    "    \n",
    "    # Quantum Components\n",
    "    n_qubits: int = 6                # Determined by PCA components\n",
    "    n_layers: int = 2\n",
    "    encoding_method: str = \"amplitude\" # 'amplitude' or 'angle'\n",
    "    \n",
    "    # Training Hyperparams\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.01\n",
    "    epochs: int = 50\n",
    "    patience: int = 10\n",
    "\n",
    "    # Preprocessing\n",
    "    reduction_method: str = \"pca\"    # 'pca' or 'lda'\n",
    "    target_dims: int = 64             # Dimensionality after reduction\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"ğŸ”’ Global seed set to {seed}\")\n",
    "\n",
    "CFG = ExperimentConfig()\n",
    "seed_everything(CFG.seed)\n",
    "os.makedirs(CFG.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2d7fa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2701,
     "status": "ok",
     "timestamp": 1760344348669,
     "user": {
      "displayName": "Michal ForgÃ³",
      "userId": "03385761472197084991"
     },
     "user_tz": -120
    },
    "id": "5d2d7fa4",
    "outputId": "d40499d8-9e61-4069-92d1-f48e0163d886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'chest-xray-pneumonia' dataset.\n",
      "Path to dataset files: /kaggle/input/chest-xray-pneumonia/chest_xray/\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "CFG.data_root = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\") + \"/chest_xray/\"\n",
    "print(\"Path to dataset files:\", CFG.data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485bfe7",
   "metadata": {
    "id": "2485bfe7"
   },
   "source": [
    "## 2. Hybrid model\n",
    "### 2.1. Classical preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7949166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 173MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading data from: /kaggle/input/chest-xray-pneumonia/chest_xray/\n",
      "   â€¢ TRAIN: Found 5216 images\n",
      "   â€¢ TEST: Found 624 images\n",
      "   â€¢ VAL: Found 16 images\n",
      "\n",
      "ğŸš€ Starting extraction with resnet50 on cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cecce6560f42e19b221762145a69f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting train:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e88235f83941cbac3001dd5b008631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting test:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325956eb6f0343f69d53c97dc3801ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting val:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extraction complete. Metadata saved to ./results/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "def get_transforms(img_size=224):\n",
    "    \"\"\"\n",
    "    Standard ImageNet normalization. \n",
    "    Using standard stats ensures the pre-trained ResNet works as intended.\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_dataloaders(config):\n",
    "    \"\"\"\n",
    "    Creates DataLoaders for Train/Test/Val using ImageFolder.\n",
    "    This replaces manual os.listdir loops.\n",
    "    \"\"\"\n",
    "    loaders = {}\n",
    "    sets = ['train', 'test', 'val']\n",
    "    \n",
    "    print(f\"ğŸ“‚ Loading data from: {config.data_root}\")\n",
    "    \n",
    "    for split in sets:\n",
    "        path = os.path.join(config.data_root, split)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"âš ï¸ Warning: Split '{split}' not found at {path}\")\n",
    "            continue\n",
    "            \n",
    "        # ImageFolder automatically handles class labels based on folder names\n",
    "        dataset = datasets.ImageFolder(root=path, transform=get_transforms())\n",
    "        \n",
    "        loaders[split] = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=False, # Important: Keep False to match features with filenames later\n",
    "            num_workers=2, # Parallel loading\n",
    "            pin_memory=True\n",
    "        )\n",
    "        print(f\"   â€¢ {split.upper()}: Found {len(dataset)} images\")\n",
    "        \n",
    "    return loaders\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps ResNet50 to output raw features instead of classification scores.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load modern V2 weights for better performance\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        self.backbone = models.resnet50(weights=weights)\n",
    "        \n",
    "        # Replace the final classification layer (fc) with Identity\n",
    "        # This allows us to get the 2048 feature vector directly\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.backbone.eval() # Set to evaluation mode (freezes BatchNorm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "def run_feature_extraction(config):\n",
    "    \"\"\"\n",
    "    The Engine: Loads data, passes it through ResNet, and saves features.\n",
    "    \"\"\"\n",
    "    device = torch.device(config.device)\n",
    "    model = FeatureExtractor().to(device)\n",
    "    loaders = get_dataloaders(config)\n",
    "    \n",
    "    # Create directory for saved features\n",
    "    save_dir = os.path.join(config.output_dir, \"features\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    print(f\"\\nğŸš€ Starting extraction with {config.backbone_name} on {device}...\")\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for speed\n",
    "        for split, loader in loaders.items():\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(tqdm(loader, desc=f\"Extracting {split}\")):\n",
    "                images = images.to(device)\n",
    "                \n",
    "                # Forward pass: Get features (Batch_Size, 2048)\n",
    "                with autocast():  # Mixed precision for speed\n",
    "                    features = model(images)\n",
    "                \n",
    "                features = model(images).cpu().numpy()\n",
    "                # Match features back to original filenames\n",
    "                # We calculate the global index based on batch size\n",
    "                start_idx = batch_idx * config.batch_size\n",
    "                \n",
    "                for i, feat in enumerate(features):\n",
    "                    global_idx = start_idx + i\n",
    "                    # Retrieve path from dataset.samples which is [(path, class_idx), ...]\n",
    "                    original_path, label_idx = loader.dataset.samples[global_idx]\n",
    "                    filename = os.path.basename(original_path)\n",
    "                    classname = loader.dataset.classes[label_idx]\n",
    "                    \n",
    "                    # Save individual feature file\n",
    "                    save_name = f\"{split}_{classname}_{filename}.npy\"\n",
    "                    save_path = os.path.join(save_dir, save_name)\n",
    "                    np.save(save_path, feat)\n",
    "                    \n",
    "                    metadata.append({\n",
    "                        'feature_path': save_path,\n",
    "                        'label': label_idx, # 0 or 1\n",
    "                        'classname': classname,\n",
    "                        'split': split,\n",
    "                        'original_path': original_path\n",
    "                    })\n",
    "    \n",
    "    # Save metadata CSV for easy loading later\n",
    "    meta_path = os.path.join(config.output_dir, \"metadata.csv\")\n",
    "    pd.DataFrame(metadata).to_csv(meta_path, index=False)\n",
    "    print(f\"âœ… Extraction complete. Metadata saved to {meta_path}\")\n",
    "    return meta_path\n",
    "\n",
    "# Execute the pipeline using our Config\n",
    "meta_csv_path = run_feature_extraction(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b2bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading raw features...\n",
      "\n",
      "âš ï¸ Fixing small validation set issue...\n",
      "   Original Val size: 16 -> New Val size: 1047\n",
      "   New Train size:    4185\n",
      "\n",
      "ğŸ”„ Fitting PCA Pipeline...\n",
      "âœ… Preprocessing complete. Data ready for Quantum Training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def load_features(config):\n",
    "    \"\"\"\n",
    "    Loads features from .npy files based on the metadata CSV.\n",
    "    \"\"\"\n",
    "    meta_path = os.path.join(config.output_dir, \"metadata.csv\")\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"Metadata CSV not found at {meta_path}. Run Step 3 first.\")\n",
    "    \n",
    "    df = pd.read_csv(meta_path)\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    X, y = {}, {}\n",
    "    \n",
    "    print(f\"ğŸ“‚ Loading raw features...\")\n",
    "    for split in splits:\n",
    "        subset = df[df['split'] == split]\n",
    "        features = [np.load(path) for path in subset['feature_path']]\n",
    "        X[split] = np.vstack(features)\n",
    "        y[split] = subset['label'].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def fix_validation_split(X, y, config):\n",
    "    \"\"\"\n",
    "    OPRAVA: SlouÄÃ­ Train (5216) a Val (16) a vytvoÅ™Ã­ novÃ© rozdÄ›lenÃ­ 80/20.\n",
    "    TÃ­m zÃ­skÃ¡me cca 1000 validaÄnÃ­ch snÃ­mkÅ¯ mÃ­sto 16.\n",
    "    \"\"\"\n",
    "    print(\"\\nâš ï¸ Fixing small validation set issue...\")\n",
    "    \n",
    "    # 1. SlouÄenÃ­\n",
    "    X_combined = np.concatenate([X['train'], X['val']])\n",
    "    y_combined = np.concatenate([y['train'], y['val']])\n",
    "    \n",
    "    # 2. NovÃ© rozdÄ›lenÃ­ (stratify zajistÃ­ sprÃ¡vnÃ½ pomÄ›r tÅ™Ã­d)\n",
    "    X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(\n",
    "        X_combined, y_combined, \n",
    "        test_size=0.2, \n",
    "        stratify=y_combined, \n",
    "        random_state=config.seed\n",
    "    )\n",
    "    \n",
    "    # 3. Aktualizace slovnÃ­kÅ¯\n",
    "    X['train'], y['train'] = X_train_new, y_train_new\n",
    "    X['val'], y['val']     = X_val_new, y_val_new\n",
    "    \n",
    "    print(f\"   Original Val size: 16 -> New Val size: {len(X_val_new)}\")\n",
    "    print(f\"   New Train size:    {len(X_train_new)}\")\n",
    "    return X, y\n",
    "\n",
    "def build_pipeline(config):\n",
    "    steps = [('scaler', StandardScaler())]\n",
    "    \n",
    "    if config.reduction_method == 'pca':\n",
    "        steps.append(('reducer', PCA(n_components=config.target_dims, random_state=config.seed)))\n",
    "    elif config.reduction_method == 'lda':\n",
    "        steps.append(('reducer', LDA(n_components=min(config.target_dims, 1))))\n",
    "    \n",
    "    if config.encoding_method == 'amplitude':\n",
    "        steps.append(('normalizer', Normalizer(norm='l2')))\n",
    "    elif config.encoding_method == 'angle':\n",
    "        steps.append(('minmax', MinMaxScaler(feature_range=(0, np.pi))))\n",
    "        \n",
    "    return Pipeline(steps)\n",
    "\n",
    "def run_classical_preprocessing(config):\n",
    "    # 1. Load Data\n",
    "    X, y = load_features(config)\n",
    "    \n",
    "    # 2. FIX DATA SPLIT (Tohle je ta klÃ­ÄovÃ¡ oprava)\n",
    "    X, y = fix_validation_split(X, y, config)\n",
    "    \n",
    "    # 3. Build & Fit Pipeline\n",
    "    pipeline = build_pipeline(config)\n",
    "    print(f\"\\nğŸ”„ Fitting {config.reduction_method.upper()} Pipeline...\")\n",
    "    \n",
    "    pipeline.fit(X['train'], y['train'])\n",
    "    \n",
    "    # 4. Transform & Save\n",
    "    X_processed = {\n",
    "        'train': pipeline.transform(X['train']),\n",
    "        'val':   pipeline.transform(X['val']),\n",
    "        'test':  pipeline.transform(X['test'])\n",
    "    }\n",
    "    \n",
    "    processed_dir = os.path.join(config.output_dir, \"processed_data\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        np.save(os.path.join(processed_dir, f\"X_{split}.npy\"), X_processed[split])\n",
    "        np.save(os.path.join(processed_dir, f\"y_{split}.npy\"), y[split])\n",
    "        \n",
    "    pipeline_path = os.path.join(config.output_dir, \"preprocessing_pipeline.joblib\")\n",
    "    joblib.dump(pipeline, pipeline_path)\n",
    "    \n",
    "    print(f\"âœ… Preprocessing complete. Data ready for Quantum Training.\")\n",
    "    return X_processed, y\n",
    "\n",
    "# SpuÅ¡tÄ›nÃ­ opravenÃ©ho kroku\n",
    "X_data, y_data = run_classical_preprocessing(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš›ï¸ Initializing Quantum Model (6 Qubits, 2 Layers)...\n",
      "ğŸš€ Starting training for 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.2 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad70f964a194bfb896598be037651b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train Loss 0.9357 | Val Loss 0.8241 | Val Acc 0.7421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fe7c29f5564435bd577555937712cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train Loss 0.8040 | Val Loss 0.7605 | Val Acc 0.7994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cc4ce0fc6b43ff848f814ca51df4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train Loss 0.7284 | Val Loss 0.6774 | Val Acc 0.7603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3978faf7705b47eead32b300b2fdde2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train Loss 0.6965 | Val Loss 0.6638 | Val Acc 0.7612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195079ab9a0b42c39d65a13a264feab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train Loss 0.6882 | Val Loss 0.6624 | Val Acc 0.7593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878df62fd3c646de9ba13b10043d31e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train Loss 0.6895 | Val Loss 0.6637 | Val Acc 0.7507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3149735fa3447d4a1ba11f8d7fbb40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train Loss 0.6861 | Val Loss 0.6611 | Val Acc 0.7612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca0b6c3ab1c4b7095a0de17bbda2d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 8: Train Loss 0.6876 | Val Loss 0.6587 | Val Acc 0.7631\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac2ff7128c744bdaf146c021ac407af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 9: Train Loss 0.6855 | Val Loss 0.6590 | Val Acc 0.7717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e894eb8d1e7c48c5bee147276e2c2c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 10: Train Loss 0.6849 | Val Loss 0.6617 | Val Acc 0.7507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cae4c4fe7e45699d00249e0eb7e88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 11: Train Loss 0.6845 | Val Loss 0.6586 | Val Acc 0.7775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bb4c101fdf4d8e9c142964f11fb90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 12: Train Loss 0.6830 | Val Loss 0.6564 | Val Acc 0.7660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ef0dc054854548ac4845fa794633a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 13: Train Loss 0.6834 | Val Loss 0.6568 | Val Acc 0.7765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e64803af39445ba3ac25b51ee5b66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 14: Train Loss 0.6846 | Val Loss 0.6555 | Val Acc 0.7755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a039ce6e38b432d839c4a35dc8fb120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 15: Train Loss 0.6835 | Val Loss 0.6555 | Val Acc 0.7689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1753256be93c42b3856a44191c1a55ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 16: Train Loss 0.6824 | Val Loss 0.6554 | Val Acc 0.7794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c903724a0e564054bf766fda5b78394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 17: Train Loss 0.6818 | Val Loss 0.6550 | Val Acc 0.7822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da1269528d14a228bffb8fa159efa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 18: Train Loss 0.6828 | Val Loss 0.6537 | Val Acc 0.7717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4667c477014846899ca07e6afebe4448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 19: Train Loss 0.6829 | Val Loss 0.6554 | Val Acc 0.7870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf99930098cb4f77a113a23e0a6cc83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 20: Train Loss 0.6832 | Val Loss 0.6532 | Val Acc 0.7775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af03cc0cd4fc43649809becd1066ff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 21: Train Loss 0.6829 | Val Loss 0.6543 | Val Acc 0.7660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac797f57f62442abb3d629a620f4e61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 22: Train Loss 0.6830 | Val Loss 0.6526 | Val Acc 0.7784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a76a8487f840ceb084d6b33f7e6b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 23: Train Loss 0.6811 | Val Loss 0.6546 | Val Acc 0.7612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146ceeadc2114116b1e1f0e107ef49a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/50:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- 1. Modular Quantum Layers ---\n",
    "\n",
    "def get_device(config):\n",
    "    \"\"\"Creates a PennyLane device with GPU acceleration.\"\"\"\n",
    "    try:\n",
    "        # This uses NVIDIA cuQuantum for massive speedups on A100/L4\n",
    "        return qml.device(\"lightning.gpu\", wires=config.n_qubits)\n",
    "    except:\n",
    "        # Fallback if GPU is not available\n",
    "        return qml.device(\"default.qubit\", wires=config.n_qubits)\n",
    "\n",
    "def embedding_layer(features, config):\n",
    "    \"\"\"\n",
    "    Encodes a BATCH of classical data into quantum states.\n",
    "    \"\"\"\n",
    "    wires = range(config.n_qubits)\n",
    "    \n",
    "    if config.encoding_method == 'amplitude':\n",
    "        # PennyLane will detect if features has a batch dimension (e.g., shape 32, 64)\n",
    "        qml.AmplitudeEmbedding(features=features, wires=wires, normalize=True, pad_with=0.0)\n",
    "        \n",
    "    elif config.encoding_method == 'angle':\n",
    "        qml.AngleEmbedding(features=features, wires=wires, rotation='Y')\n",
    "\n",
    "def ansatz_layer(params, config):\n",
    "    \"\"\"\n",
    "    Optimized Ansatz for batched inputs.\n",
    "    \"\"\"\n",
    "    n_wires = config.n_qubits\n",
    "    \n",
    "    for l in range(config.n_layers):\n",
    "        # Parameterized Rotations\n",
    "        for w in range(n_wires):\n",
    "            # Rot is compatible with batching when using broadcastable params\n",
    "            qml.Rot(params[l, w, 0], params[l, w, 1], params[l, w, 2], wires=w)\n",
    "            \n",
    "        # Entangling Layer\n",
    "        for w in range(n_wires):\n",
    "            qml.CNOT(wires=[w, (w + 1) % n_wires])\n",
    "\n",
    "# --- 2. The QNode Builder ---\n",
    "\n",
    "def build_qnode(config):\n",
    "    dev = get_device(config)\n",
    "    \n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"adjoint\", cache=True)\n",
    "    def qnode(inputs, params):\n",
    "        embedding_layer(inputs, config)\n",
    "        ansatz_layer(params, config)\n",
    "        # When inputs is batched, this returns a batch of expectation values\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "    return qnode \n",
    "\n",
    "# --- 3. Training Engine ---\n",
    "\n",
    "def train_quantum_model(config, X_data, y_data):\n",
    "    \"\"\"\n",
    "    Main training loop using Autograd.\n",
    "    \"\"\"\n",
    "    print(f\"\\nâš›ï¸ Initializing Quantum Model ({config.n_qubits} Qubits, {config.n_layers} Layers)...\")\n",
    "    \n",
    "    # Initialize QNode\n",
    "    qnode = build_qnode(config)\n",
    "    \n",
    "    # Initialize Parameters (Random weights)\n",
    "    # Shape matching our manual ansatz: (L, N_wires, 3)\n",
    "    param_shape = (config.n_layers, config.n_qubits, 3)\n",
    "    params = pnp.random.uniform(0, 2*np.pi, size=param_shape, requires_grad=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    opt = qml.AdamOptimizer(stepsize=config.learning_rate)\n",
    "    \n",
    "    # Cost Function (MSE)\n",
    "    def cost_fn(params, x_batch, y_batch):\n",
    "        # PASS THE WHOLE BATCH AT ONCE\n",
    "        # preds will now have shape (batch_size,)\n",
    "        preds = qnode(x_batch, params)\n",
    "        \n",
    "        # Transform labels: 0/1 -> -1/1 to match PauliZ output\n",
    "        targets = pnp.array([1 if y == 1 else -1 for y in y_batch], requires_grad=False)\n",
    "        \n",
    "        # Mean Squared Error\n",
    "        return pnp.mean((preds - targets) ** 2)\n",
    "\n",
    "    # Tracking\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_params = None\n",
    "    \n",
    "    # Data Setup\n",
    "    X_train, y_train = X_data['train'], y_data['train']\n",
    "    X_val, y_val = X_data['val'], y_data['val']\n",
    "    \n",
    "    batch_size = config.batch_size\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    print(f\"ğŸš€ Starting training for {config.epochs} epochs...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[perm]\n",
    "        y_train = y_train[perm]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Batch Loop\n",
    "        with tqdm(total=n_batches, desc=f\"Epoch {epoch+1}/{config.epochs}\", leave=False) as pbar:\n",
    "            for i in range(n_batches):\n",
    "                batch_idx = slice(i * batch_size, (i + 1) * batch_size)\n",
    "                X_batch = X_train[batch_idx]\n",
    "                y_batch = y_train[batch_idx]\n",
    "                \n",
    "                # Step\n",
    "                params, loss = opt.step_and_cost(lambda p: cost_fn(p, X_batch, y_batch), params)\n",
    "                epoch_loss += loss\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'loss': f\"{loss:.4f}\"})\n",
    "        \n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = cost_fn(params, X_val, y_val)\n",
    "        \n",
    "        # Val Accuracy (threshold at 0.0 because PauliZ is [-1, 1])\n",
    "        # Faster Validation in the training loop\n",
    "        val_preds_raw = qnode(X_val, params) # Pass all validation data at once if memory allows\n",
    "        val_preds = (val_preds_raw > 0).astype(int) \n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "        \n",
    "        history['train_loss'].append(float(avg_train_loss))\n",
    "        history['val_loss'].append(float(val_loss))\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"   Epoch {epoch+1}: Train Loss {avg_train_loss:.4f} | Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params.copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= config.patience:\n",
    "            print(f\"â¹ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"âœ… Training done in {train_time:.1f}s. Best Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_params, qnode, history\n",
    "\n",
    "# --- 4. Evaluation Function ---\n",
    "\n",
    "def evaluate_model(config, params, qnode, X_test, y_test):\n",
    "    print(\"\\nğŸ“Š Evaluating on Test Set...\")\n",
    "    preds_raw = np.array([qnode(x, params) for x in X_test])\n",
    "    \n",
    "    # Map [-1, 1] -> [0, 1]\n",
    "    probs = (preds_raw + 1) / 2\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, preds),\n",
    "        'f1_score': f1_score(y_test, preds, average='macro'),\n",
    "        'auc_score': roc_auc_score(y_test, probs)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Test Accuracy: {metrics['accuracy']:.2%}\")\n",
    "    print(f\"   Test F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"   Test AUC:      {metrics['auc_score']:.4f}\")\n",
    "    return metrics, preds, probs\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# 1. Train\n",
    "best_params, qnode, history = train_quantum_model(CFG, X_data, y_data)\n",
    "\n",
    "# 2. Evaluate\n",
    "test_metrics, test_preds, test_probs = evaluate_model(CFG, best_params, qnode, X_data['test'], y_data['test'])\n",
    "\n",
    "# 3. Save\n",
    "results_path = os.path.join(CFG.output_dir, \"quantum_results.json\")\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'config': {k: str(v) for k, v in CFG.__dict__.items()},\n",
    "        'metrics': test_metrics,\n",
    "        'history': history\n",
    "    }, f, indent=4)\n",
    "np.save(os.path.join(CFG.output_dir, \"best_params.npy\"), best_params)\n",
    "print(f\"ğŸ’¾ Saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ac3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# NastavenÃ­ profesionÃ¡lnÃ­ho vzhledu grafÅ¯\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12, 'figure.dpi': 300}) # 300 DPI pro tiskovou kvalitu\n",
    "\n",
    "def plot_training_history(history, save_dir):\n",
    "    \"\"\"\n",
    "    VykreslÃ­ vÃ½voj chyby (Loss) a pÅ™esnosti (Accuracy) bÄ›hem trÃ©ninku.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Graf Chyby (Loss)\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='TrÃ©novacÃ­ chyba', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r--', label='ValidaÄnÃ­ chyba', linewidth=2)\n",
    "    ax1.set_title('VÃ½voj chybovÃ© funkce (Loss)', fontweight='bold')\n",
    "    ax1.set_xlabel('Epocha')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Graf PÅ™esnosti (Accuracy)\n",
    "    ax2.plot(epochs, history['val_acc'], 'g-', label='ValidaÄnÃ­ pÅ™esnost', linewidth=2)\n",
    "    ax2.set_title('VÃ½voj pÅ™esnosti na validaÄnÃ­ sadÄ›', fontweight='bold')\n",
    "    ax2.set_xlabel('Epocha')\n",
    "    ax2.set_ylabel('PÅ™esnost (0-1)')\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, \"training_history.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    print(f\"ğŸ“ˆ Graf trÃ©ninku uloÅ¾en: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_evaluation_metrics(y_true, y_pred, y_probs, save_dir):\n",
    "    \"\"\"\n",
    "    VykreslÃ­ Matici zÃ¡mÄ›n a ROC kÅ™ivku.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Matice zÃ¡mÄ›n (Confusion Matrix)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                xticklabels=['Normal', 'Pneumonia'],\n",
    "                yticklabels=['Normal', 'Pneumonia'],\n",
    "                annot_kws={\"size\": 14, \"weight\": \"bold\"})\n",
    "    ax1.set_title('Matice zÃ¡mÄ›n (Confusion Matrix)', fontweight='bold')\n",
    "    ax1.set_ylabel('SkuteÄnÃ¡ tÅ™Ã­da')\n",
    "    ax1.set_xlabel('PredikovanÃ¡ tÅ™Ã­da')\n",
    "    \n",
    "    # 2. ROC KÅ™ivka\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC kÅ™ivka (AUC = {roc_auc:.2f})')\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate (1 - Specificita)')\n",
    "    ax2.set_ylabel('True Positive Rate (Senzitivita)')\n",
    "    ax2.set_title('ROC KÅ™ivka', fontweight='bold')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, \"evaluation_metrics.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Grafy metrik uloÅ¾eny: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- SpuÅ¡tÄ›nÃ­ vizualizace ---\n",
    "\n",
    "# PouÅ¾Ã­vÃ¡me promÄ›nnÃ© z pÅ™edchozÃ­ch krokÅ¯:\n",
    "# history (z kroku 5)\n",
    "# y_data['test'] (z kroku 4/5 - skuteÄnÃ© hodnoty)\n",
    "# test_preds (z kroku 5 - predikovanÃ© 0/1)\n",
    "# test_probs (z kroku 5 - pravdÄ›podobnosti)\n",
    "\n",
    "print(f\"=== VIZUALIZACE VÃSLEDKÅ® PRO SOÄŒ ===\")\n",
    "plot_training_history(history, CFG.output_dir)\n",
    "plot_evaluation_metrics(y_data['test'], test_preds, test_probs, CFG.output_dir)\n",
    "\n",
    "# Bonus: VÃ½pis finÃ¡lnÃ­ch ÄÃ­sel pro text prÃ¡ce\n",
    "cm = confusion_matrix(y_data['test'], test_preds)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nğŸ“ Data pro tabulku vÃ½sledkÅ¯ v SOÄŒ:\")\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"PoÄet testovacÃ­ch snÃ­mkÅ¯: {len(y_data['test'])}\")\n",
    "print(f\"TP (SprÃ¡vnÄ› Pneumonie):   {tp}\")\n",
    "print(f\"TN (SprÃ¡vnÄ› ZdravÃ­):      {tn}\")\n",
    "print(f\"FP (FaleÅ¡nÃ½ poplach):     {fp}\")\n",
    "print(f\"FN (PÅ™ehlÃ©dnutÃ¡ nemoc):   {fn}\")\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"Senzitivita (Recall):     {sensitivity:.4f}\")\n",
    "print(f\"Specificita:              {specificity:.4f}\")\n",
    "print(f\"AUC:                      {roc_auc_score(y_data['test'], test_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cceec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ğŸ” Single Image Evaluation\n",
    "# ==========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pennylane as qml\n",
    "\n",
    "print(\"=== Single Image Test Mode ===\")\n",
    "img_path = input(\"Enter path to an image (.jpeg/.jpg/.png): \").strip()\n",
    "metadata_path = \"./data/features/metadata.csv\"\n",
    "\n",
    "# --- Load metadata to identify label ---\n",
    "meta = pd.read_csv(metadata_path)\n",
    "meta['image_ref'] = meta['image_path'].astype(str).str.lower()\n",
    "img_name = os.path.basename(img_path).lower()\n",
    "true_label = None\n",
    "\n",
    "for _, row in meta.iterrows():\n",
    "    if img_name in os.path.basename(row['image_ref']):\n",
    "        true_label = \"PNEUMONIA\" if int(row['label']) == 1 else \"NORMAL\"\n",
    "        break\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "img = Image.open(img_path).convert('L').resize((224,224))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f\"Input Image: {os.path.basename(img_path)}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "\n",
    "# --- Load same CNN as used in feature extraction ---\n",
    "model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))  # remove final FC\n",
    "model.eval()\n",
    "\n",
    "# --- Define same preprocessing as training pipeline ---\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# --- Run CNN feature extraction ---\n",
    "img_rgb = Image.open(img_path).convert('RGB')\n",
    "input_tensor = preprocess(img_rgb).unsqueeze(0)  # shape (1,3,224,224)\n",
    "with torch.no_grad():\n",
    "    features_cnn = model(input_tensor).squeeze().numpy()  # shape (2048,)\n",
    "\n",
    "img_flat = features_cnn.reshape(1, -1)\n",
    "\n",
    "\n",
    "# --- Load reducer from training (e.g., PCA joblib) ---\n",
    "reducer_path = f\"./results/{REDUCTION_METHOD}_reducer_{ACTUAL_DIMS}d.joblib\"\n",
    "if os.path.exists(reducer_path):\n",
    "    reducer = joblib.load(reducer_path)\n",
    "    img_reduced = reducer.transform(img_flat)\n",
    "else:\n",
    "    print(f\"âš ï¸ Reducer not found at {reducer_path}, reusing mean/std from training.\")\n",
    "    img_reduced = img_flat[:, :ACTUAL_DIMS]\n",
    "\n",
    "# --- Prepare for quantum encoding ---\n",
    "if ENCODING == \"amplitude\":\n",
    "    vec = np.zeros(2**N_QUBITS)\n",
    "    vec[:len(img_reduced[0])] = img_reduced[0]\n",
    "    q_input = vec / np.linalg.norm(vec)\n",
    "else:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    sc = MinMaxScaler((0, 2*np.pi))\n",
    "    q_input = sc.fit_transform(img_reduced)[0]\n",
    "\n",
    "# --- Define quantum circuit ---\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS, shots=None)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode_predict(x, theta):\n",
    "    if ENCODING == \"amplitude\":\n",
    "        qml.AmplitudeEmbedding(x, wires=range(N_QUBITS), normalize=True, pad_with=0.0)\n",
    "    else:\n",
    "        for i, val in enumerate(x):\n",
    "            qml.RY(val, wires=i)\n",
    "    p = theta.reshape(N_LAYERS, N_QUBITS, 3)\n",
    "    for l in range(N_LAYERS):\n",
    "        for w in range(N_QUBITS):\n",
    "            qml.RX(p[l,w,0], wires=w)\n",
    "            qml.RY(p[l,w,1], wires=w)\n",
    "            qml.RZ(p[l,w,2], wires=w)\n",
    "        for w in range(N_QUBITS):\n",
    "            qml.CNOT(wires=[w, (w+1)%N_QUBITS])\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# --- Load trained quantum parameters ---\n",
    "param_path = f\"./results/params_{ENCODING}_{REDUCTION_METHOD}{ACTUAL_DIMS}d_{N_LAYERS}L.npy\"\n",
    "if not os.path.exists(param_path):\n",
    "    raise FileNotFoundError(f\"Trained quantum parameters not found: {param_path}\")\n",
    "params_final = np.load(param_path)\n",
    "\n",
    "# --- Predict ---\n",
    "prediction = qnode_predict(q_input, params_final)\n",
    "prob_pneumonia = (1 + prediction) / 2\n",
    "pred_label = \"PNEUMONIA\" if prob_pneumonia > 0.5 else \"NORMAL\"\n",
    "\n",
    "print(\"\\nğŸ§  Model Prediction:\")\n",
    "print(f\"  â†’ Predicted: {pred_label} (probability={prob_pneumonia:.3f})\")\n",
    "if true_label:\n",
    "    print(f\"  â†’ Actual Label: {true_label}\")\n",
    "    print(\"âœ… Correct!\" if pred_label == true_label else \"âŒ Incorrect.\")\n",
    "\n",
    "plt.title(f\"Prediction: {pred_label}  |  Actual: {true_label or 'Unknown'}\")\n",
    "plt.axis('off')\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
